{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333af3cf",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685656ce",
   "metadata": {},
   "source": [
    "# Web scraping and analysis\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called BeautifulSoup to collect the data from the web. Once you've collected your data and saved it into a local .csv file you should start with your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72946d25",
   "metadata": {},
   "source": [
    "## Scraping data from Skytrax\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use Python and BeautifulSoup to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd7cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.2-cp39-cp39-win_amd64.whl (153 kB)\n",
      "     -------------------------------------- 153.3/153.3 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pillow in c:\\users\\krish\\anaconda3\\lib\\site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\krish\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\krish\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\krish\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\krish\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\krish\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde2764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from wordcloud import WordCloud \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99ea963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "   ---> 100 total reviews\n",
      "Scraping page 2\n",
      "   ---> 200 total reviews\n",
      "Scraping page 3\n",
      "   ---> 300 total reviews\n",
      "Scraping page 4\n",
      "   ---> 400 total reviews\n",
      "Scraping page 5\n",
      "   ---> 500 total reviews\n",
      "Scraping page 6\n",
      "   ---> 600 total reviews\n",
      "Scraping page 7\n",
      "   ---> 700 total reviews\n",
      "Scraping page 8\n",
      "   ---> 800 total reviews\n",
      "Scraping page 9\n",
      "   ---> 900 total reviews\n",
      "Scraping page 10\n",
      "   ---> 1000 total reviews\n",
      "Scraping page 11\n",
      "   ---> 1100 total reviews\n",
      "Scraping page 12\n",
      "   ---> 1200 total reviews\n",
      "Scraping page 13\n",
      "   ---> 1300 total reviews\n",
      "Scraping page 14\n",
      "   ---> 1400 total reviews\n",
      "Scraping page 15\n",
      "   ---> 1500 total reviews\n",
      "Scraping page 16\n",
      "   ---> 1600 total reviews\n",
      "Scraping page 17\n",
      "   ---> 1700 total reviews\n",
      "Scraping page 18\n",
      "   ---> 1800 total reviews\n",
      "Scraping page 19\n",
      "   ---> 1900 total reviews\n",
      "Scraping page 20\n",
      "   ---> 2000 total reviews\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "\n",
    "# Initialize an empty list to store the data that you scrape\n",
    "data = []\n",
    "\n",
    "# Setting the initial page number and the increment that you want to use to paginate through the webpage\n",
    "page_num = 1\n",
    "page_incr = 1\n",
    "page_size = 100\n",
    "# maximum number of pages to be scraped\n",
    "max_pages = 20\n",
    "\n",
    "# Set the URL of the webpage to be scraped \n",
    "paginated_url = f\"{url}/page/{page_num}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "# A while loop to paginate through the webpage and scrape the data\n",
    "while page_num <= max_pages:\n",
    "\n",
    "    print(f\"Scraping page {page_num}\")\n",
    "\n",
    "    # A GET request to the paginated URL\n",
    "    response = requests.get(paginated_url)\n",
    "\n",
    "    # Parsing the response using BeautifulSoup\n",
    "    parsed_content = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Finding all the elements on the page that contain the data to be scraped\n",
    "    elements = parsed_content.find_all(\"div\",class_ = \"body\")\n",
    " # Looping through the elements and extract the data that you want to scrape\n",
    "    for element in elements:\n",
    "        header = element.find(\"h2\",class_ = \"text_header\").text.replace(\"\\n\", \" \")\n",
    "        sub_header = element.find(\"h3\",class_ = \"text_sub_header\").text.replace(\"\\n\", \" \")\n",
    "        content = element.find(\"div\",class_ = \"text_content\").text.replace(\"\\n\", \" \")\n",
    "        \n",
    "        data.append([header,sub_header,content])\n",
    "\n",
    "    # Increasing the page number and setting the paginated URL to the new page\n",
    "    page_num += page_incr\n",
    "    paginated_url = f\"{url}/page/{page_num}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    print(f\"   ---> {len(data)} total reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd462d9",
   "metadata": {},
   "source": [
    "The loops above collected 2000 reviews by iterating through the paginated pages on the website.\n",
    "\n",
    "The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90fef305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REVIEW</th>\n",
       "      <th>PERSONAL INFO</th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"flights changed with no cost\"</td>\n",
       "      <td>William Jackson (Spain) 23rd May 2023</td>\n",
       "      <td>Not Verified |  Easy check in on the way to He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Cheap, quick and efficient\"</td>\n",
       "      <td>A Warten (Chile) 23rd May 2023</td>\n",
       "      <td>Online check in worked fine. Quick security ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"the worst major European airline\"</td>\n",
       "      <td>E Michaels (United Kingdom) 22nd May 2023</td>\n",
       "      <td>. The BA first lounge at Terminal 5 was a  zoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"do not think the fare was worth the money\"</td>\n",
       "      <td>Steve Bennett (United Kingdom) 22nd May 2023</td>\n",
       "      <td>Not Verified | Paid a quick visit to Nice yest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"BA is on the skids downhill\"</td>\n",
       "      <td>N Mayle (United States) 19th May 2023</td>\n",
       "      <td>Words fail to describe this last awful flight ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>\"experience has really declined\"</td>\n",
       "      <td>G Mantimo (Canada) 23rd August 2016</td>\n",
       "      <td>✅ Verified Review |  The British Airways exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>\"BA has declined significantly\"</td>\n",
       "      <td>Richard Brown (New Zealand) 22nd August 2016</td>\n",
       "      <td>Flew Malta to London. First the plus points. G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>\"First Class is a total wate of money\"</td>\n",
       "      <td>Bill Atkins (United Kingdom) 21st August 2016</td>\n",
       "      <td>Philadelphia to London Heathrow with British A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>\"every time I complain about the breakfast\"</td>\n",
       "      <td>H Lowe (United Kingdom) 19th August 2016</td>\n",
       "      <td>Upgraded on the outbound flight from London to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>\"customer services representatives were unhelp...</td>\n",
       "      <td>S Green (United Kingdom) 19th August 2016</td>\n",
       "      <td>We booked the return from our honeymoon with B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 REVIEW  \\\n",
       "0                        \"flights changed with no cost\"   \n",
       "1                          \"Cheap, quick and efficient\"   \n",
       "2                   \"the worst major European airline\"    \n",
       "3           \"do not think the fare was worth the money\"   \n",
       "4                         \"BA is on the skids downhill\"   \n",
       "...                                                 ...   \n",
       "1995                   \"experience has really declined\"   \n",
       "1996                    \"BA has declined significantly\"   \n",
       "1997             \"First Class is a total wate of money\"   \n",
       "1998        \"every time I complain about the breakfast\"   \n",
       "1999  \"customer services representatives were unhelp...   \n",
       "\n",
       "                                        PERSONAL INFO  \\\n",
       "0               William Jackson (Spain) 23rd May 2023   \n",
       "1                      A Warten (Chile) 23rd May 2023   \n",
       "2           E Michaels (United Kingdom) 22nd May 2023   \n",
       "3        Steve Bennett (United Kingdom) 22nd May 2023   \n",
       "4               N Mayle (United States) 19th May 2023   \n",
       "...                                               ...   \n",
       "1995              G Mantimo (Canada) 23rd August 2016   \n",
       "1996     Richard Brown (New Zealand) 22nd August 2016   \n",
       "1997    Bill Atkins (United Kingdom) 21st August 2016   \n",
       "1998         H Lowe (United Kingdom) 19th August 2016   \n",
       "1999        S Green (United Kingdom) 19th August 2016   \n",
       "\n",
       "                                                CONTENT  \n",
       "0     Not Verified |  Easy check in on the way to He...  \n",
       "1     Online check in worked fine. Quick security ch...  \n",
       "2     . The BA first lounge at Terminal 5 was a  zoo...  \n",
       "3     Not Verified | Paid a quick visit to Nice yest...  \n",
       "4     Words fail to describe this last awful flight ...  \n",
       "...                                                 ...  \n",
       "1995  ✅ Verified Review |  The British Airways exper...  \n",
       "1996  Flew Malta to London. First the plus points. G...  \n",
       "1997  Philadelphia to London Heathrow with British A...  \n",
       "1998  Upgraded on the outbound flight from London to...  \n",
       "1999  We booked the return from our honeymoon with B...  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coverting the list data into a dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = [\"REVIEW\",\"PERSONAL INFO\",\"CONTENT\"]\n",
    "\n",
    "#Removing unwanted text(first text preprocessing)\n",
    "df.replace(re.compile(r'\\s*✅ Trip Verified \\|\\s*'), '', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f510161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\krish\\OneDrive\\Desktop\\ba\\BA_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb3a047d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not Verified |  Easy check in on the way to He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Online check in worked fine. Quick security ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>. The BA first lounge at Terminal 5 was a  zoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not Verified | Paid a quick visit to Nice yest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Words fail to describe this last awful flight ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>The British Airways experience has really decl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Flew Malta to London. First the plus points. G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Philadelphia to London Heathrow with British A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Upgraded on the outbound flight from London to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>We booked the return from our honeymoon with B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                CONTENT\n",
       "0     Not Verified |  Easy check in on the way to He...\n",
       "1     Online check in worked fine. Quick security ch...\n",
       "2     . The BA first lounge at Terminal 5 was a  zoo...\n",
       "3     Not Verified | Paid a quick visit to Nice yest...\n",
       "4     Words fail to describe this last awful flight ...\n",
       "...                                                 ...\n",
       "1995  The British Airways experience has really decl...\n",
       "1996  Flew Malta to London. First the plus points. G...\n",
       "1997  Philadelphia to London Heathrow with British A...\n",
       "1998  Upgraded on the outbound flight from London to...\n",
       "1999  We booked the return from our honeymoon with B...\n",
       "\n",
       "[2000 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis_df = df.drop([\"REVIEW\",\"PERSONAL INFO\"], axis=1)\n",
    "sentiment_analysis_df.replace(re.compile(r'\\s*✅ Verified Review \\|\\s*'), '', inplace=True)\n",
    "sentiment_analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8471c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_df.to_csv(\"sentiment_content.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5949b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
